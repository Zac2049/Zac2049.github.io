<!DOCTYPE html>
<html lang="zh-cn"
  dir="ltr">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width">








    






<link rel="icon" type="image/ico" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="192x192" href="http://localhost:1313/android-chrome-192x192.png">
<link rel="apple-touch-icon" sizes="180x180" href="http://localhost:1313/apple-touch-icon.png">

<meta name="description" content=""/>



<title>
    
    浅识扩散模型 | Hedgehugger Land
    
</title>

<link rel="canonical" href="http://localhost:1313/posts/%E6%B5%85%E8%AF%86%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/"/>

<meta property="og:url" content="http://localhost:1313/posts/%E6%B5%85%E8%AF%86%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/">
  <meta property="og:site_name" content="Hedgehugger Land">
  <meta property="og:title" content="浅识扩散模型">
  <meta property="og:description" content="1. DDPM的核心思想 # DDPM是一种基于概率的生成模型，其核心思想是通过模拟一个前向扩散过程（逐渐向数据添加噪声）和一个逆向去噪过程（从噪声恢复数据）来生成高质量样本。DDPM的训练目标是学习逆向过程的概率分布，使其能够从纯噪声中逐步生成逼真的样本。
前向过程（Forward Process）：从真实数据 $ x_0 $ 开始，逐步添加高斯噪声，经过 $ T $ 步后，数据变成近似的各向同性高斯噪声。 逆向过程（Reverse Process）：从高斯噪声 $ x_T $ 开始，逐步去噪，恢复到原始数据 $ x_0 $。 DDPM的关键是逆向过程的概率分布 $ p_\theta(x_{t-1}|x_t) $ 难以直接建模，因此通过训练一个神经网络来逼近它，并通过优化一个变分下界（variational lower bound）来学习参数。
2. 前向扩散过程的推导 # 2.1 前向过程的定义 # 前向过程是一个马尔可夫链，定义为从真实数据 $ x_0 $ 到噪声 $ x_T $ 的逐步加噪过程。每一步添加一个高斯噪声，形式为：
$$ q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t} x_{t-1}, \beta_t I) $$ 其中：
$ x_t $ 是第 $ t $ 步的加噪数据。 $ \beta_t \in (0, 1) $ 是第 $ t $ 步的噪声调度参数，控制每一步添加的噪声量。 $ \sqrt{1-\beta_t} x_{t-1} $ 是均值，意味着数据被稍微缩放。 $ \beta_t I $ 是方差，表示添加的噪声强度。 直观来说，每一步都在 $ x_{t-1} $ 的基础上乘以一个小于1的系数（使信号衰减），并加上一个高斯噪声。">
  <meta property="og:locale" content="zh_cn">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-05-09T10:00:00+08:00">
    <meta property="article:modified_time" content="2025-05-09T10:00:00+08:00">
    <meta property="article:tag" content="技术">
    <meta property="article:tag" content="机器学习">







<link rel="stylesheet" href="/assets/combined.min.57df9e7d692553f966cff4593ede6b53efb74da234876bacffeac774bd572e53.css" media="all">











    




</head>





<body class="auto">

  <div class="content">
    <header>
      

<div class="header">

    

    <h1 class="header-title">
        <a href="http://localhost:1313/">Hedgehugger Land</a>
    </h1>

    <div class="header-menu">
        

        
        

        <p
            class="small ">
            <a href="/" >
                /home
            </a>
        </p>
        

        <p
            class="small ">
            <a href="/posts" >
                /posts
            </a>
        </p>
        

        <p
            class="small ">
            <a href="/tags" >
                /tags
            </a>
        </p>
        

        <p
            class="small ">
            <a href="/about" >
                /about
            </a>
        </p>
        
        
    </div>

    

</div>

    </header>

    <main class="main">
      




<div class="breadcrumbs"><a href="/">Home</a><span class="breadcrumbs-separator">/</span><a href="/posts/">Posts</a><span class="breadcrumbs-separator">/</span>
        <a href="/posts/%E6%B5%85%E8%AF%86%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/">浅识扩散模型</a></div>


<div >
  <article>
    <header class="single-intro-container">
        
        <h1 class="single-title">浅识扩散模型</h1>
        
        <div class="single-subsummary">
          
          <div>
            
            <p class="single-date">
              <time datetime="2025-05-09T10:00:00&#43;08:00">May 9, 2025</time>
            </p>
          </div>
        </div>
        
    </header>
    
    <div class="single-content">
      <h2 class="heading" id="1-ddpm的核心思想">
  1. DDPM的核心思想
  <a class="anchor" href="#1-ddpm%e7%9a%84%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3">#</a>
</h2>
<p>DDPM是一种基于概率的生成模型，其核心思想是通过模拟一个<strong>前向扩散过程</strong>（逐渐向数据添加噪声）和一个<strong>逆向去噪过程</strong>（从噪声恢复数据）来生成高质量样本。DDPM的训练目标是学习逆向过程的概率分布，使其能够从纯噪声中逐步生成逼真的样本。</p>
<ul>
<li><strong>前向过程</strong>（Forward Process）：从真实数据 $ x_0 $ 开始，逐步添加高斯噪声，经过 $ T $ 步后，数据变成近似的各向同性高斯噪声。</li>
<li><strong>逆向过程</strong>（Reverse Process）：从高斯噪声 $ x_T $ 开始，逐步去噪，恢复到原始数据 $ x_0 $。</li>
</ul>
<p>DDPM的关键是逆向过程的概率分布 $ p_\theta(x_{t-1}|x_t) $ 难以直接建模，因此通过训练一个神经网络来逼近它，并通过优化一个变分下界（variational lower bound）来学习参数。</p>
<hr>
<h2 class="heading" id="2-前向扩散过程的推导">
  2. 前向扩散过程的推导
  <a class="anchor" href="#2-%e5%89%8d%e5%90%91%e6%89%a9%e6%95%a3%e8%bf%87%e7%a8%8b%e7%9a%84%e6%8e%a8%e5%af%bc">#</a>
</h2>
<h3 class="heading" id="21-前向过程的定义">
  2.1 前向过程的定义
  <a class="anchor" href="#21-%e5%89%8d%e5%90%91%e8%bf%87%e7%a8%8b%e7%9a%84%e5%ae%9a%e4%b9%89">#</a>
</h3>
<p>前向过程是一个马尔可夫链，定义为从真实数据 $ x_0 $ 到噪声 $ x_T $ 的逐步加噪过程。每一步添加一个高斯噪声，形式为：</p>

$$ q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t} x_{t-1}, \beta_t I) $$


<p>其中：</p>
<ul>
<li>$ x_t $ 是第 $ t $ 步的加噪数据。</li>
<li>$ \beta_t \in (0, 1) $ 是第 $ t $ 步的噪声调度参数，控制每一步添加的噪声量。</li>
<li>$ \sqrt{1-\beta_t} x_{t-1} $ 是均值，意味着数据被稍微缩放。</li>
<li>$ \beta_t I $ 是方差，表示添加的噪声强度。</li>
</ul>
<p>直观来说，每一步都在 $ x_{t-1} $ 的基础上乘以一个小于1的系数（使信号衰减），并加上一个高斯噪声。</p>
<h3 class="heading" id="22-多步前向过程">
  2.2 多步前向过程
  <a class="anchor" href="#22-%e5%a4%9a%e6%ad%a5%e5%89%8d%e5%90%91%e8%bf%87%e7%a8%8b">#</a>
</h3>
<p>由于前向过程是马尔可夫链，我们可以写出从 $ x_0 $ 到 $ x_t $ 的联合分布：</p>

$$ q(x_{1:t} | x_0) = \prod_{s=1}^t q(x_s | x_{s-1}) $$


<p>更重要的是，我们可以直接计算从 $ x_0 $ 到任意 $ x_t $ 的边缘分布 $ q(x_t | x_0) $。DDPM中通过巧妙的参数化，得到了一个非常方便的闭合形式。</p>
<p>定义：

$$ \alpha_t = 1 - \beta_t, \quad \bar{\alpha}_t = \prod_{s=1}^t \alpha_s $$

</p>
<p>那么，$ q(x_t | x_0) $ 可以表示为：</p>

$$ q(x_t | x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t) I) $$


<p><strong>推导过程</strong>：</p>
<ul>
<li>假设

  $$ x_{t-1} \sim \mathcal{N}(\sqrt{\bar{\alpha}_{t-1}} x_0, (1 - \bar{\alpha}_{t-1}) I) $$
  

我们需要计算 $ x_t $</li>
<li>根据前向过程定义，$ x_t = \sqrt{\alpha_t} x_{t-1} + \sqrt{\beta_t} \epsilon $，其中 $ \epsilon \sim \mathcal{N}(0, I) $。
将 
  $$ x_{t-1} = \sqrt{\bar{\alpha}_{t-1}} x_0 + \sqrt{1 - \bar{\alpha}_{t-1}} \epsilon_{t-1} $$ 
代入，得到：

  $$ x_t = \sqrt{\alpha_t} (\sqrt{\bar{\alpha}_{t-1}} x_0 + \sqrt{1 - \bar{\alpha}_{t-1}} \epsilon_{t-1}) + \sqrt{\beta_t} \epsilon $$
  


  $$ = \sqrt{\alpha_t \bar{\alpha}_{t-1}} x_0 + \sqrt{\alpha_t (1 - \bar{\alpha}_{t-1})} \epsilon_{t-1} + \sqrt{\beta_t} \epsilon $$
  
</li>
<li>由于 $ \bar{\alpha}_t = \alpha_t \bar{\alpha}_{t-1} $
，均值项为 $ \sqrt{\bar{\alpha}_t} x_0 $。</li>
<li>方差项需要合并两个高斯噪声的方差：

  $$ \text{Var}(x_t) = \alpha_t (1 - \bar{\alpha}_{t-1}) + \beta_t = 1 - \alpha_t \bar{\alpha}_{t-1} = 1 - \bar{\alpha}_t $$
  
</li>
</ul>
<p>因此，$ x_t $ 可以直接从 $ x_0 $ 采样：</p>

$$ x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon, \quad \epsilon \sim \mathcal{N}(0, I) $$


<p>这个公式非常重要，因为它允许我们从真实数据 $ x_0 $ 直接生成任意时间步 $ t $ 的加噪数据，而无需逐步模拟整个马尔可夫链。</p>
<hr>
<h2 class="heading" id="3-逆向去噪过程的建模">
  3. 逆向去噪过程的建模
  <a class="anchor" href="#3-%e9%80%86%e5%90%91%e5%8e%bb%e5%99%aa%e8%bf%87%e7%a8%8b%e7%9a%84%e5%bb%ba%e6%a8%a1">#</a>
</h2>
<h3 class="heading" id="31-逆向过程的定义">
  3.1 逆向过程的定义
  <a class="anchor" href="#31-%e9%80%86%e5%90%91%e8%bf%87%e7%a8%8b%e7%9a%84%e5%ae%9a%e4%b9%89">#</a>
</h3>
<p>逆向过程也是一个马尔可夫链，定义为从噪声 $ x_T $ 逐步去噪到数据 $ x_0 $：</p>

$$ p_\theta(x_{0:T}) = p(x_T) \prod_{t=1}^T p_\theta(x_{t-1} | x_t) $$


<p>其中：</p>
<ul>
<li>$ p(x_T) = \mathcal{N}(0, I) $，假设 $ T $ 足够大，$ x_T $ 近似于各向同性高斯噪声。</li>
<li>$ p_\theta(x_{t-1} | x_t) $ 是参数化的逆向转移概率，通常假设为高斯分布：</li>
</ul>

$$ p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t)) $$


<p>DDPM的目标是学习参数 $ \theta $，使得 $ p_\theta(x_0) $ 尽可能接近真实数据分布 $ q(x_0) $。</p>
<h3 class="heading" id="32-真实逆向分布">
  3.2 真实逆向分布
  <a class="anchor" href="#32-%e7%9c%9f%e5%ae%9e%e9%80%86%e5%90%91%e5%88%86%e5%b8%83">#</a>
</h3>
<p>理论上，真实逆向分布 $ q(x_{t-1} | x_t, x_0) $ 可以通过贝叶斯公式计算：</p>

$$ q(x_{t-1} | x_t, x_0) = \frac{q(x_t | x_{t-1}, x_0) q(x_{t-1} | x_0)}{q(x_t | x_0)} $$


<p>由于 $ q(x_t | x_{t-1}, x_0) = q(x_t | x_{t-1}) $，代入前向过程的定义：</p>

$$ q(x_{t-1} | x_t, x_0) \propto \mathcal{N}(x_t; \sqrt{\alpha_t} x_{t-1}, \beta_t I) \cdot \mathcal{N}(x_{t-1}; \sqrt{\bar{\alpha}_{t-1}} x_0, (1 - \bar{\alpha}_{t-1}) I) $$


<p>经过高斯分布的乘积运算（略去繁琐推导），可以得到 $ q(x_{t-1} | x_t, x_0) $ 也是高斯分布，其均值和方差为：</p>

$$ \mu_q = \frac{\sqrt{\bar{\alpha}_{t-1}} \beta_t}{1 - \bar{\alpha}_t} x_0 + \frac{\sqrt{\alpha_t} (1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} x_t $$



$$ \Sigma_q = \frac{(1 - \bar{\alpha}_{t-1}) \beta_t}{1 - \bar{\alpha}_t} I $$


<p>然而，这个真实逆向分布依赖于 $ x_0 $，而在采样时我们无法直接访问 $ x_0 $。因此，DDPM通过神经网络来逼近这个分布。</p>
<hr>
<h2 class="heading" id="4-损失函数的推导">
  4. 损失函数的推导
  <a class="anchor" href="#4-%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0%e7%9a%84%e6%8e%a8%e5%af%bc">#</a>
</h2>
<h3 class="heading" id="41-变分下界elbo">
  4.1 变分下界（ELBO）
  <a class="anchor" href="#41-%e5%8f%98%e5%88%86%e4%b8%8b%e7%95%8celbo">#</a>
</h3>
<p>DDPM的训练目标是最大化数据似然 $ p_\theta(x_0) $。直接优化似然很困难，因此我们优化其对数似然的变分下界（Evidence Lower Bound, ELBO）。ELBO的推导基于KL散度：</p>

$$ \log p_\theta(x_0) \geq \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \frac{p_\theta(x_{0:T})}{q(x_{1:T}|x_0)} \right] $$


<p>展开后，ELBO可以写为：</p>

$$ L = \mathbb{E}_q \left[ \log p_\theta(x_0 | x_1) + \sum_{t=2}^T \log \frac{p_\theta(x_{t-1} | x_t)}{q(x_{t-1} | x_t, x_0)} + \log \frac{p(x_T)}{q(x_T | x_0)} \right] $$


<p>将其拆分为三部分：</p>
<ol>
<li><strong>重构项</strong>：$ L_0 = -\log p_\theta(x_0 | x_1) $，表示从 $ x_1 $ 重构 $ x_0 $ 的误差。</li>
<li><strong>去噪匹配项</strong>：$ L_{t-1} = D_{KL}(q(x_{t-1} | x_t, x_0) || p_\theta(x_{t-1} | x_t)) $，表示逆向分布与真实逆向分布的KL散度。</li>
<li><strong>先验匹配项</strong>：$ L_T = D_{KL}(q(x_T | x_0) || p(x_T)) $，表示 $ x_T $ 与标准高斯分布的匹配。</li>
</ol>
<p>当 $ T $ 足够大时，$ q(x_T | x_0) \approx \mathcal{N}(0, I) $，因此 $ L_T \approx 0 $，可以忽略。</p>
<h3 class="heading" id="42-简化损失函数">
  4.2 简化损失函数
  <a class="anchor" href="#42-%e7%ae%80%e5%8c%96%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0">#</a>
</h3>
<p>DDPM的关键创新在于简化 $ L_{t-1} $。假设逆向过程的均值为：</p>

$$ \mu_\theta(x_t, t) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_\theta(x_t, t) \right) $$


<p>其中 $ \epsilon_\theta(x_t, t) $ 是神经网络预测的噪声。方差通常取为 $ \Sigma_\theta = \beta_t I $ 或其他调度形式。</p>
<p>通过代入 $ x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon $ 
，并计算KL散度，DDPM发现优化 $ L_{t-1} $ 等价于最小化以下损失：</p>

$$ L_{t-1} \propto \mathbb{E}_{x_0, \epsilon} \left[ \left\| \epsilon - \epsilon_\theta(\sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon, t) \right\|^2 \right] $$


<p>最终，DDPM的简化损失函数（忽略常数项）为：</p>

$$ L_{\text{simple}} = \mathbb{E}_{t \sim [1,T], x_0, \epsilon} \left[ \left\| \epsilon - \epsilon_\theta(x_t, t) \right\|^2 \right] $$


<p>其中：</p>
<ul>
<li>$ t \sim \text{Uniform}(1, T) $，随机采样时间步。</li>
<li>$ x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon $，通过前向过程生成。</li>
<li>$ \epsilon_\theta(x_t, t) $ 是神经网络预测的噪声。</li>
</ul>
<p>这个损失函数的直觉是：训练神经网络预测加到 $ x_0 $ 上的噪声 $ \epsilon $，从而间接学习逆向去噪过程。</p>
<hr>
<h2 class="heading" id="5-采样过程的推导">
  5. 采样过程的推导
  <a class="anchor" href="#5-%e9%87%87%e6%a0%b7%e8%bf%87%e7%a8%8b%e7%9a%84%e6%8e%a8%e5%af%bc">#</a>
</h2>
<h3 class="heading" id="51-逆向采样公式">
  5.1 逆向采样公式
  <a class="anchor" href="#51-%e9%80%86%e5%90%91%e9%87%87%e6%a0%b7%e5%85%ac%e5%bc%8f">#</a>
</h3>
<p>训练完成后，我们使用逆向过程从噪声 $ x_T \sim \mathcal{N}(0, I) $ 开始采样，逐步生成 $ x_{T-1}, x_{T-2}, \dots, x_0 $。逆向过程的每一步为：</p>

$$ p_\theta(x_{t-1} | x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t)) $$


<p>均值 $ \mu_\theta $ 由神经网络预测的噪声 $ \epsilon_\theta $ 计算：</p>

$$ \mu_\theta(x_t, t) = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_\theta(x_t, t) \right) $$


<p>方差 $ \Sigma_\theta $ 通常取为 $ \beta_t I $ 或其他调度（如DDIM中使用的确定性采样）。</p>
<p>采样公式为：</p>

$$ x_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \epsilon_\theta(x_t, t) \right) + \sqrt{\beta_t} z, \quad z \sim \mathcal{N}(0, I) $$


<h3 class="heading" id="52-采样算法">
  5.2 采样算法
  <a class="anchor" href="#52-%e9%87%87%e6%a0%b7%e7%ae%97%e6%b3%95">#</a>
</h3>
<p>完整的采样算法如下：</p>
<ol>
<li>初始化 $ x_T \sim \mathcal{N}(0, I) $。</li>
<li>对于 $ t = T, T-1, \dots, 1 $：
<ul>
<li>计算 $ \epsilon_\theta(x_t, t) $。</li>
<li>计算均值 $ \mu_\theta(x_t, t) $。</li>
<li>采样 $ x_{t-1} \sim \mathcal{N}(\mu_\theta(x_t, t), \beta_t I) $。</li>
</ul>
</li>
<li>返回 $ x_0 $。</li>
</ol>
<h3 class="heading" id="53-改进ddim">
  5.3 改进：DDIM
  <a class="anchor" href="#53-%e6%94%b9%e8%bf%9bddim">#</a>
</h3>
<p>DDPM的采样过程需要 $ T $ 步，计算成本高。Denoising Diffusion Implicit Models (DDIM) 提出了一种确定性采样方法，通过修改逆向过程，允许更少的采样步数（例如 50 或 100 步），显著加速生成过程，同时保持生成质量。</p>
<hr>
<h2 class="heading" id="6-总结与直觉">
  6. 总结与直觉
  <a class="anchor" href="#6-%e6%80%bb%e7%bb%93%e4%b8%8e%e7%9b%b4%e8%a7%89">#</a>
</h2>
<h3 class="heading" id="61-损失函数的直觉">
  6.1 损失函数的直觉
  <a class="anchor" href="#61-%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0%e7%9a%84%e7%9b%b4%e8%a7%89">#</a>
</h3>
<ul>
<li>DDPM的损失函数本质上是让神经网络预测前向过程中添加的噪声 $ \epsilon $。</li>
<li>通过最小化预测噪声与真实噪声的均方误差，模型学会了如何从 $ x_t $ 恢复 $ x_{t-1} $，从而间接逼近逆向分布。</li>
</ul>
<h3 class="heading" id="62-采样过程的直觉">
  6.2 采样过程的直觉
  <a class="anchor" href="#62-%e9%87%87%e6%a0%b7%e8%bf%87%e7%a8%8b%e7%9a%84%e7%9b%b4%e8%a7%89">#</a>
</h3>
<ul>
<li>采样过程类似于“从噪声中雕刻数据”。每一步，模型根据当前加噪状态 $ x_t $ 预测噪声，并利用它来估计更接近真实数据的 $ x_{t-1} $。</li>
<li>整个过程是一个逐步去噪的过程，最终从纯噪声生成逼真的样本。</li>
</ul>
<h3 class="heading" id="63-数学美感">
  6.3 数学美感
  <a class="anchor" href="#63-%e6%95%b0%e5%ad%a6%e7%be%8e%e6%84%9f">#</a>
</h3>
<ul>
<li>前向过程的闭合形式（直接从 $ x_0 $ 到 $ x_t $）大大简化了训练和采样。</li>
<li>变分下界的简化使得优化目标变得直观且易于实现。</li>
</ul>
<hr>
<h2 class="heading" id="7-代码实现简要">
  7. 代码实现（简要）
  <a class="anchor" href="#7-%e4%bb%a3%e7%a0%81%e5%ae%9e%e7%8e%b0%e7%ae%80%e8%a6%81">#</a>
</h2>
<p>以下是一个简化的DDPM训练和采样的伪代码，供参考：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 训练</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">train_ddpm</span>(model, data_loader, num_epochs, T, betas):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(num_epochs):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> x0 <span style="color:#f92672">in</span> data_loader:
</span></span><span style="display:flex;"><span>            t <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randint(<span style="color:#ae81ff">1</span>, T<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>, (x0<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>],))  <span style="color:#75715e"># 随机时间步</span>
</span></span><span style="display:flex;"><span>            epsilon <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn_like(x0)  <span style="color:#75715e"># 随机噪声</span>
</span></span><span style="display:flex;"><span>            alpha_bar_t <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>prod(<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> betas[:t])  <span style="color:#75715e"># 计算 $ \bar{\alpha}_t $</span>
</span></span><span style="display:flex;"><span>            xt <span style="color:#f92672">=</span> sqrt(alpha_bar_t) <span style="color:#f92672">*</span> x0 <span style="color:#f92672">+</span> sqrt(<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> alpha_bar_t) <span style="color:#f92672">*</span> epsilon  <span style="color:#75715e"># 前向过程</span>
</span></span><span style="display:flex;"><span>            loss <span style="color:#f92672">=</span> mse_loss(epsilon, model(xt, t))  <span style="color:#75715e"># 预测噪声的损失</span>
</span></span><span style="display:flex;"><span>            optimize(loss)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 采样</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sample_ddpm</span>(model, num_samples, T, betas):
</span></span><span style="display:flex;"><span>    xt <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(num_samples, <span style="color:#f92672">...</span>)  <span style="color:#75715e"># 从高斯噪声开始</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> range(T, <span style="color:#ae81ff">0</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>):
</span></span><span style="display:flex;"><span>        epsilon_theta <span style="color:#f92672">=</span> model(xt, t)  <span style="color:#75715e"># 预测噪声</span>
</span></span><span style="display:flex;"><span>        alpha_t <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> betas[t]
</span></span><span style="display:flex;"><span>        mu_theta <span style="color:#f92672">=</span> (xt <span style="color:#f92672">-</span> betas[t] <span style="color:#f92672">/</span> sqrt(<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> alpha_bar_t) <span style="color:#f92672">*</span> epsilon_theta) <span style="color:#f92672">/</span> sqrt(alpha_t)
</span></span><span style="display:flex;"><span>        xt <span style="color:#f92672">=</span> mu_theta <span style="color:#f92672">+</span> sqrt(betas[t]) <span style="color:#f92672">*</span> torch<span style="color:#f92672">.</span>randn_like(xt)  <span style="color:#75715e"># 逆向采样</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> xt
</span></span></code></pre></div>
    </div>
  </article>

  

  

  
  

<div class="single-pagination">
    <hr />

    <div class="flexnowrap">

        <div class="single-pagination-prev">
            
            <div class="single-pagination-container-prev">
                <div class="single-pagination-text">←</div>
                <div class="single-pagination-text">
                    <a href="/posts/my-first-post/">
                        我的第一篇文章
                    </a>
                </div>
            </div>
            
        </div>

        <div class="single-pagination-next">
            
        </div>

    </div>

    <hr />
</div>



  

  

  
  <div class="back-to-top">
    <a href="#top">
      back to top
    </a>
  </div>
  

</div>


    </main>
  </div>

  
  





    




  <footer>
    

    
    





    




    
    <p>Powered by
        <a href="https://gohugo.io/">Hugo</a>
        and
        <a href="https://github.com/tomfran/typo">tomfran/typo</a>
    </p>
    
    
    


  </footer>

  
  <link rel="stylesheet" 
  href="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css">
  
<script defer 
  src="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.js"></script>

<script defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/contrib/auto-render.min.js"
  onload="renderMathInElement(document.body);"></script>

<script>
  document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(document.body, {
      delimiters: [
        { left: "$$", right: "$$", display: true },
        { left: "$", right: "$", display: false },
        { left: "\\[", right: "\\]", display: true },
        { left: "\\(", right: "\\)", display: false },
        { left: "\[", right: "\]", display: true },
        { left: "\(", right: "\)", display: false },
      ]
    });
  });
</script>
  
</body>

<script src="/js/theme-switch.js"></script>
<script defer src="/js/copy-code.js"></script>
</html>
