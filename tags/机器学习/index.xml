<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>机器学习 on Hedgehugger Land</title>
    <link>http://localhost:1313/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 机器学习 on Hedgehugger Land</description>
    <generator>Hugo</generator>
    <language>zh-cn</language>
    <lastBuildDate>Fri, 09 May 2025 10:00:00 +0800</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>浅识扩散模型</title>
      <link>http://localhost:1313/posts/%E6%B5%85%E8%AF%86%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/</link>
      <pubDate>Fri, 09 May 2025 10:00:00 +0800</pubDate>
      <guid>http://localhost:1313/posts/%E6%B5%85%E8%AF%86%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/</guid>
      <description>&lt;p&gt;好的！下面我会详细讲解去噪扩散概率模型（Denoising Diffusion Probabilistic Models, DDPM）的损失函数和采样过程的推导，尽量清晰、结构化，并包含必要的数学公式。为了便于理解，我会从DDPM的核心思想开始，逐步推导损失函数（即训练目标）和采样过程（即生成过程），并在必要处解释背后的直觉。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 class=&#34;heading&#34; id=&#34;1-ddpm的核心思想&#34;&gt;&#xA;  1. DDPM的核心思想&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#1-ddpm%e7%9a%84%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;DDPM是一种基于概率的生成模型，其核心思想是通过模拟一个&lt;strong&gt;前向扩散过程&lt;/strong&gt;（逐渐向数据添加噪声）和一个&lt;strong&gt;逆向去噪过程&lt;/strong&gt;（从噪声恢复数据）来生成高质量样本。DDPM的训练目标是学习逆向过程的概率分布，使其能够从纯噪声中逐步生成逼真的样本。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;前向过程&lt;/strong&gt;（Forward Process）：从真实数据 $ x_0 $ 开始，逐步添加高斯噪声，经过 $ T $ 步后，数据变成近似的各向同性高斯噪声。&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;逆向过程&lt;/strong&gt;（Reverse Process）：从高斯噪声 $ x_T $ 开始，逐步去噪，恢复到原始数据 $ x_0 $。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;DDPM的关键是逆向过程的概率分布 $ p_\theta(x_{t-1}|x_t) $ 难以直接建模，因此通过训练一个神经网络来逼近它，并通过优化一个变分下界（variational lower bound）来学习参数。&lt;/p&gt;&#xA;&lt;hr&gt;&#xA;&lt;h2 class=&#34;heading&#34; id=&#34;2-前向扩散过程的推导&#34;&gt;&#xA;  2. 前向扩散过程的推导&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#2-%e5%89%8d%e5%90%91%e6%89%a9%e6%95%a3%e8%bf%87%e7%a8%8b%e7%9a%84%e6%8e%a8%e5%af%bc&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;h3 class=&#34;heading&#34; id=&#34;21-前向过程的定义&#34;&gt;&#xA;  2.1 前向过程的定义&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#21-%e5%89%8d%e5%90%91%e8%bf%87%e7%a8%8b%e7%9a%84%e5%ae%9a%e4%b9%89&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;前向过程是一个马尔可夫链，定义为从真实数据 $ x_0 $ 到噪声 $ x_T $ 的逐步加噪过程。每一步添加一个高斯噪声，形式为：&lt;/p&gt;&#xA;&#xA;$$ q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t} x_{t-1}, \beta_t I) $$&#xA;&#xA;&#xA;&lt;p&gt;其中：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;$ x_t $ 是第 $ t $ 步的加噪数据。&lt;/li&gt;&#xA;&lt;li&gt;$ \beta_t \in (0, 1) $ 是第 $ t $ 步的噪声调度参数，控制每一步添加的噪声量。&lt;/li&gt;&#xA;&lt;li&gt;$ \sqrt{1-\beta_t} x_{t-1} $ 是均值，意味着数据被稍微缩放。&lt;/li&gt;&#xA;&lt;li&gt;$ \beta_t I $ 是方差，表示添加的噪声强度。&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;直观来说，每一步都在 $ x_{t-1} $ 的基础上乘以一个小于1的系数（使信号衰减），并加上一个高斯噪声。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
